# pqn_vdn_ff is suggested for MPE (faster and good enough), but pqn_vdn_rnn is also available
"TOTAL_TIMESTEPS": 20e6
"NUM_ENVS": 256
"NUM_STEPS": 101
"MEMORY_WINDOW": 0 # extra steps in the rnn horizon from previous episodes 
"HIDDEN_SIZE": 256
"NUM_LAYERS": 2
"NORM_INPUTS": False
"NORM_TYPE": "layer_norm"
"EPS_START": 1.0
"EPS_FINISH": 0.1
"EPS_DECAY": 0.1
"MAX_GRAD_NORM": 1
"NUM_MINIBATCHES": 16
"NUM_EPOCHS": 4
"LR": 0.0005
"LR_LINEAR_DECAY": False
"GAMMA": 0.9
"LAMBDA": 0.3

# ENV
"ENV_NAME": "rware"
"ENV_KWARGS": {
  "num_agents": 3,
  "num_cells": 6,
  "max_steps": 100,
  "action_type": "Discrete",
  "robotarium": {
    "number_of_robots": 3,
    "show_figure": False,
    "sim_in_real_time": False,
  },
  "controller": {
    "controller": "clf_uni_position",
    "barrier_fn": "robust_barriers",
  },
  "update_frequency": 60
}

# evaluate
"TEST_DURING_TRAINING": True
"TEST_INTERVAL": 0.05 # as a fraction of updates, i.e. log every 5% of training process
"TEST_NUM_STEPS": 101
"TEST_NUM_ENVS": 64 # number of episodes to average over, can affect performance

#"ALG_NAME": "pqn_vdn_cnn" # if you want to change the name of the algo in the metrics